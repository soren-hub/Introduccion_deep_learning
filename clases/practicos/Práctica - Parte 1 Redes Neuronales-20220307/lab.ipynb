{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# librer√≠as\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Sequential\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import functional as F\n",
    "import numpy\n",
    "\n",
    "# seed fija para experimentos\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([768, 8]), torch.Size([768, 1]))"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape,Y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entradas:\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x0000015EC9456B50>\n",
      "Salidas:\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "Observe que hay 768 ejemplos y 8 atributos:\n",
      "77\n",
      "Observe que hay 768 etiquetas\n",
      "torch.Size([768, 1])\n"
     ]
    }
   ],
   "source": [
    "#cargar los datos\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# separamos en variables de entrada (X) y salidas (Y) \n",
    "inputs = torch.from_numpy(dataset[:,0:8]).float()\n",
    "X = DataLoader(inputs, batch_size=10, shuffle=True)\n",
    "Y = torch.reshape(torch.from_numpy(dataset[:,8]).float(), (-1,1))\n",
    "data =  DataLoader(torch.from_numpy(dataset).float(), batch_size=10, shuffle=True)\n",
    "\n",
    "\n",
    "print(\"Entradas:\")\n",
    "print(X)\n",
    "print(\"Salidas:\")\n",
    "print(Y)\n",
    "print(\"Observe que hay 768 ejemplos y 8 atributos:\")\n",
    "print(len(X))\n",
    "print(\"Observe que hay 768 etiquetas\")\n",
    "print((Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class FFNN(Module):\n",
    "\n",
    "  def __init__(self, d0=8, d1=12):\n",
    "    super(FFNN, self).__init__()\n",
    "    \n",
    "    #self.fc_1 = Linear(d0,d1)\n",
    "    #self.fc_2 = Linear(d1,d0) \n",
    "    #self.fc_out = Linear(d0,1)\n",
    "    #self.relu = nn.ReLU()\n",
    "    #self.sigmoid = nn.Sigmoid()\n",
    "    relu=nn.ReLU\n",
    "    \n",
    "    layer_list=[\n",
    "          (\"input\",Linear(d0,d1)),\n",
    "          ('relu1', relu()),\n",
    "          ('linear', Linear(d1,d0)),\n",
    "          ('relu2', relu()),\n",
    "          ('output',Linear(d0,1)),\n",
    "    #      ('sigmoid', sigmoide()),    \n",
    "    ]\n",
    "    layerDict = OrderedDict(layer_list)\n",
    "    self.layers = Sequential(layerDict)\n",
    "    self.apply(self.init_weights)\n",
    "    self.optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    output = self.layers.input(x)\n",
    "    output = self.layers.relu1(output)\n",
    "    output = self.layers.linear(output)\n",
    "    output = self.layers.relu2(output)\n",
    "    output = self.layers.output(output)\n",
    "    output = torch.sigmoid(output)\n",
    "    return output\n",
    "  \n",
    "  def evaluate(self,predict):\n",
    "    return (predict > 0.5).float()\n",
    "  \n",
    "  def accuracy(self,y_test,x_test): \n",
    "    y_pred = self.evaluate(x_test)\n",
    "    acc=accuracy_score(y_test.numpy(), y_pred.numpy())\n",
    "    print(f'Accuracy: {acc}')\n",
    "  \n",
    "  @torch.no_grad()\n",
    "  def init_weights(self,m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "      nn.init.uniform_(m.weight)\n",
    "      m.bias.fill_(0.0)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FFNN(\n",
       "  (layers): Sequential(\n",
       "    (input): Linear(in_features=8, out_features=12, bias=True)\n",
       "    (relu1): ReLU()\n",
       "    (linear): Linear(in_features=12, out_features=8, bias=True)\n",
       "    (relu2): ReLU()\n",
       "    (output): Linear(in_features=8, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=FFNN()\n",
    "criterion = nn.BCELoss()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=1000\n",
    "model_params=[]\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    #iteration_loss,iteration_accuracy = 0.,0.\n",
    "    #for i, batch_data in enumerate(data):\n",
    "    #    batch_size = len(batch_data)\n",
    "    #    input_batch, output_batch = batch_data[:,0:8], batch_data[:,8].reshape(-1,1)\n",
    "    #    prediction = model(input_batch)\n",
    "    #    loss = criterion(prediction, output_batch)\n",
    "    #    optimizer.zero_grad()\n",
    "    #    loss.backward()\n",
    "    #    optimizer.step()\n",
    "    #    iteration_loss += loss.item()\n",
    "    input, output = inputs,Y\n",
    "    prediction = model.forward(input)\n",
    "    pre=model.parameters()\n",
    "    loss = criterion(prediction, output)\n",
    "    model.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    model.optimizer.step()\n",
    "    pos=model.parameters()\n",
    "    model_params.append((pos,pre))\n",
    "\n",
    "    if epoch % 10==0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            prediction = model(inputs)\n",
    "            output = model.evaluate(prediction)\n",
    "            accuracy = accuracy_score(Y.numpy(), output.numpy())\n",
    "        print(f'Epoch {epoch+1}/{epochs} loss: {loss.item()} Accuracy {accuracy:.5f}')\n",
    "        \n",
    "confusion_matrix(Y.numpy(), output.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 11/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 21/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 31/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 41/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 51/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 61/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 71/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 81/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 91/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 101/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 111/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 121/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 131/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 141/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 151/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 161/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 171/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 181/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 191/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 201/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 211/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 221/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 231/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 241/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 251/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 261/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 271/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 281/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 291/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 301/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 311/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 321/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 331/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 341/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 351/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 361/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 371/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 381/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 391/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 401/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 411/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 421/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 431/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 441/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 451/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 461/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 471/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 481/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 491/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 501/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 511/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 521/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 531/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 541/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 551/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 561/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 571/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 581/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 591/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 601/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 611/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 621/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 631/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 641/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 651/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 661/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 671/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 681/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 691/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 701/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 711/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 721/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 731/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 741/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 751/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 761/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 771/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 781/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 791/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 801/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 811/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 821/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 831/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 841/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 851/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 861/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 871/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 881/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 891/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 901/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 911/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 921/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 931/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 941/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 951/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 961/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 971/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 981/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 991/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1001/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1011/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1021/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1031/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1041/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1051/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1061/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1071/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1081/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1091/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1101/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1111/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1121/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1131/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1141/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1151/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1161/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1171/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1181/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1191/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1201/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1211/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1221/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1231/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1241/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1251/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1261/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1271/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1281/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1291/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1301/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1311/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1321/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1331/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1341/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1351/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1361/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1371/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1381/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1391/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1401/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1411/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1421/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1431/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1441/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1451/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1461/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1471/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1481/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1491/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1501/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1511/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1521/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1531/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1541/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1551/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1561/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1571/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1581/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1591/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1601/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1611/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1621/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1631/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1641/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1651/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1661/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1671/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1681/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1691/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1701/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1711/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1721/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1731/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1741/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1751/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1761/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1771/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1781/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1791/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1801/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1811/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1821/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1831/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1841/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1851/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1861/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1871/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1881/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1891/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1901/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1911/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1921/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1931/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1941/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1951/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1961/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1971/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1981/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 1991/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2001/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2011/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2021/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2031/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2041/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2051/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2061/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2071/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2081/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2091/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2101/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2111/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2121/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2131/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2141/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2151/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2161/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2171/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2181/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2191/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2201/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2211/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2221/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2231/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2241/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2251/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2261/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2271/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2281/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2291/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2301/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2311/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2321/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2331/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2341/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2351/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2361/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2371/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2381/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2391/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2401/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2411/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2421/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2431/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2441/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2451/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2461/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2471/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2481/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2491/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2501/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2511/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2521/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2531/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2541/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2551/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2561/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2571/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2581/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2591/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2601/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2611/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2621/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2631/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2641/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2651/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2661/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2671/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2681/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2691/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2701/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2711/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2721/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2731/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2741/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2751/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2761/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2771/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2781/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2791/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2801/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2811/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2821/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2831/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2841/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2851/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2861/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2871/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2881/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2891/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2901/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2911/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2921/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2931/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2941/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2951/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2961/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2971/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2981/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 2991/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3001/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3011/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3021/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3031/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3041/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3051/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3061/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3071/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3081/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3091/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3101/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3111/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3121/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3131/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3141/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3151/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3161/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3171/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3181/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3191/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3201/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3211/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3221/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3231/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3241/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3251/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3261/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3271/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3281/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3291/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3301/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3311/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3321/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3331/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3341/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3351/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3361/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3371/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3381/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3391/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3401/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3411/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3421/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3431/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3441/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3451/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3461/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3471/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3481/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3491/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3501/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3511/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3521/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3531/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3541/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3551/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3561/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3571/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3581/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3591/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3601/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3611/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3621/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3631/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3641/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3651/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3661/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3671/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3681/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3691/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3701/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3711/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3721/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3731/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3741/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3751/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3761/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3771/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3781/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3791/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3801/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3811/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3821/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3831/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3841/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3851/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3861/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3871/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3881/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3891/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3901/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3911/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3921/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3931/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3941/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3951/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3961/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3971/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3981/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 3991/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4001/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4011/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4021/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4031/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4041/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4051/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4061/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4071/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4081/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4091/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4101/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4111/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4121/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4131/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4141/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4151/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4161/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4171/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4181/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4191/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4201/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4211/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4221/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4231/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4241/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4251/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4261/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4271/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4281/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4291/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4301/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4311/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4321/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4331/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4341/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4351/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4361/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4371/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4381/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4391/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4401/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4411/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4421/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4431/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4441/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4451/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4461/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4471/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4481/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4491/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4501/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4511/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4521/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4531/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4541/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4551/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4561/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4571/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4581/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4591/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4601/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4611/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4621/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4631/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4641/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4651/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4661/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4671/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4681/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4691/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4701/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4711/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4721/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4731/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4741/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4751/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4761/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4771/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4781/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4791/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4801/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4811/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4821/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4831/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4841/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4851/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4861/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4871/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4881/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4891/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4901/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4911/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4921/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4931/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4941/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4951/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4961/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4971/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4981/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 4991/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5001/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5011/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5021/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5031/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5041/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5051/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5061/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5071/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5081/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5091/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5101/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5111/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5121/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5131/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5141/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5151/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5161/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5171/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5181/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5191/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5201/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5211/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5221/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5231/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5241/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5251/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5261/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5271/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5281/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5291/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5301/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5311/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5321/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5331/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5341/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5351/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5361/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5371/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5381/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5391/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5401/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5411/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5421/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5431/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5441/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5451/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5461/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5471/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5481/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5491/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5501/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5511/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5521/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5531/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5541/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5551/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5561/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5571/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5581/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5591/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5601/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5611/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5621/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5631/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5641/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5651/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5661/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5671/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5681/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5691/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5701/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5711/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5721/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5731/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5741/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5751/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5761/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5771/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5781/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5791/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5801/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5811/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5821/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5831/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5841/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5851/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5861/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5871/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5881/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5891/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5901/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5911/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5921/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5931/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5941/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5951/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5961/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5971/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5981/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 5991/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6001/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6011/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6021/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6031/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6041/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6051/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6061/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6071/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6081/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6091/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6101/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6111/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6121/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6131/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6141/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6151/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6161/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6171/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6181/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6191/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6201/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6211/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6221/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6231/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6241/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6251/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6261/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6271/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6281/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6291/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6301/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6311/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6321/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6331/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6341/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6351/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6361/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6371/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6381/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6391/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6401/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6411/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6421/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6431/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6441/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6451/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6461/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6471/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6481/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6491/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6501/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6511/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6521/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6531/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6541/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6551/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6561/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6571/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6581/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6591/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6601/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6611/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6621/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6631/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6641/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6651/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6661/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6671/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6681/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6691/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6701/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6711/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6721/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6731/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6741/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6751/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6761/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6771/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6781/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6791/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6801/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6811/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6821/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6831/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6841/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6851/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6861/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6871/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6881/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6891/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6901/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6911/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6921/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6931/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6941/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6951/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6961/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6971/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6981/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 6991/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7001/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7011/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7021/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7031/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7041/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7051/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7061/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7071/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7081/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7091/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7101/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7111/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7121/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7131/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7141/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7151/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7161/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7171/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7181/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7191/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7201/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7211/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7221/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7231/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7241/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7251/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7261/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7271/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7281/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7291/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7301/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7311/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7321/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7331/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7341/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7351/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7361/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7371/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7381/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7391/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7401/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7411/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7421/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7431/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7441/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7451/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7461/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7471/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7481/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7491/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7501/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7511/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7521/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7531/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7541/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7551/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7561/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7571/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7581/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7591/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7601/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7611/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7621/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7631/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7641/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7651/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7661/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7671/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7681/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7691/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7701/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7711/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7721/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7731/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7741/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7751/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7761/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7771/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7781/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7791/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7801/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7811/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7821/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7831/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7841/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7851/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7861/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7871/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7881/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7891/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7901/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7911/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7921/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7931/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7941/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7951/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7961/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7971/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7981/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 7991/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8001/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8011/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8021/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8031/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8041/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8051/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8061/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8071/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8081/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8091/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8101/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8111/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8121/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8131/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8141/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8151/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8161/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8171/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8181/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8191/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8201/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8211/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8221/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8231/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8241/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8251/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8261/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8271/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8281/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8291/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8301/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8311/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8321/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8331/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8341/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8351/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8361/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8371/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8381/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8391/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8401/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8411/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8421/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8431/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8441/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8451/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8461/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8471/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8481/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8491/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8501/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8511/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8521/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8531/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8541/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8551/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8561/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8571/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8581/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8591/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8601/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8611/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8621/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8631/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8641/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8651/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8661/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8671/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8681/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8691/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8701/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8711/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8721/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8731/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8741/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8751/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8761/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8771/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8781/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8791/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8801/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8811/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8821/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8831/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8841/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8851/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8861/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8871/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8881/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8891/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8901/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8911/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8921/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8931/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8941/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8951/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8961/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8971/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8981/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 8991/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9001/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9011/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9021/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9031/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9041/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9051/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9061/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9071/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9081/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9091/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9101/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9111/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9121/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9131/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9141/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9151/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9161/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9171/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9181/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9191/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9201/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9211/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9221/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9231/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9241/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9251/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9261/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9271/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9281/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9291/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9301/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9311/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9321/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9331/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9341/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9351/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9361/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9371/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9381/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9391/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9401/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9411/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9421/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9431/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9441/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9451/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9461/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9471/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9481/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9491/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9501/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9511/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9521/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9531/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9541/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9551/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9561/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9571/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9581/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9591/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9601/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9611/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9621/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9631/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9641/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9651/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9661/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9671/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9681/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9691/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9701/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9711/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9721/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9731/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9741/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9751/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9761/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9771/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9781/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9791/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9801/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9811/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9821/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9831/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9841/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9851/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9861/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9871/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9881/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9891/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9901/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9911/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9921/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9931/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9941/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9951/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9961/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9971/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9981/10000 loss: 65.10416412353516 Accuracy 0.34896\n",
      "Epoch 9991/10000 loss: 65.10416412353516 Accuracy 0.34896\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#otra forma de defnir el modelo\n",
    "relu=nn.ReLU\n",
    "model1 = Sequential(OrderedDict([\n",
    "          (\"input\", Linear(8, 12)),\n",
    "          ('relu1', relu()),\n",
    "          ('linear', nn.Linear(12,8)),\n",
    "          ('relu2', relu()),\n",
    "          ('output', nn.Linear(8, 1)),\n",
    "        ]))\n",
    "\n",
    "@torch.no_grad()\n",
    "def init_weights(m):\n",
    "  if isinstance(m, nn.Linear):\n",
    "    nn.init.uniform_(m.weight)\n",
    "    m.bias.fill_(0.0)\n",
    "model1.apply(init_weights)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model1.parameters())\n",
    "m = nn.Sigmoid()\n",
    "\n",
    "epochs=10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model1.train()\n",
    "    input, output = inputs,Y\n",
    "    optimizer.zero_grad()\n",
    "    prediction = model1(input)\n",
    "    loss = criterion(m(prediction), output)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    if epoch % 10==0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = m(model(inputs))\n",
    "            predicted = (output > 0.5).float()\n",
    "            total += Y.shape[0]\n",
    "            correct += int((predicted == Y).sum())\n",
    "        print(f'Epoch {epoch+1}/{epochs} loss: {loss.item()} Accuracy { correct / total:.5f}')\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f8cf1ecaca8bf1a25c3dee625425e172027cada71133da2a1e2438c70452260"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('intre_dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
