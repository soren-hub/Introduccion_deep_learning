{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase Práctica 06\n",
    "\n",
    "# CNN - Redes Neuronales Convolucionales\n",
    "\n",
    "# Reconocimiento de dígitos escritos a mano\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Una demostración popular del uso de las técnicas de Deep Learning es el reconocimiento de objetos en imagenes. El \"Hola mundo\" del reconocimiento de objetos consiste en utilizar el conjunto de datos MNIST para el reconocimiento de dígitos a mano.\n",
    "\n",
    "En esta clase práctica se va a desarrollar un modelo de aprendizaje profundo para lograr un rendimiento casi moderno en el reconocimiento de dígitos manuscritos (MNIST) en Python, utilizando la biblioteca de aprendizaje profundo Keras. \n",
    "\n",
    "MNIST es una base de datos desarrollada por Yann LeCun, Corinna Cortes y Christopher Burges para evaluar modelos de aprendizaje automático sobre el problema de clasificación de dígitos manuscritos.\n",
    "\n",
    "La base de datos se construyó a partir de una serie de documentos escaneados disponibles del Instituto Nacional de Estándares y Tecnología (NIST). Aquí es de donde viene el nombre, base de datos NIST Modied o MNIST.\n",
    "\n",
    "Se tomaron imágenes de dígitos de una gran variedad de documentos escaneados, normalizados en tamaño y centrados. Esto lo convierte en una excelente base de datos para evaluar modelos, lo que le permite al desarrollador enfocarse en el aprendizaje automático con muy poca limpieza de datos o preparación requerida. Cada imagen es un cuadrado de 28x28 píxeles (784 píxeles en total). Se utiliza una división estándar del conjunto de datos para evaluar y comparar modelos, se usan 60,000 imágenes para entrenar un modelo y un conjunto separado de 10,000 imágenes para probarla.\n",
    "\n",
    "Es una tarea de reconocimiento de dígitos. Como tal, hay 10 dígitos (0 a 9) o 10 clases para predecir. Se puede lograr un error de predicción en el estado del arte de aproximadamente 0.2% con redes neuronales convolucionales profundas.\n",
    "\n",
    "En esta clase práctica, se realizará la clasificación de la base de datos MNIST usando redes neuronales clásicas y redes neuronales profundas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargando la base de datos MNIST en Keras\n",
    "\n",
    "Keras proporciona un método conveniente para cargar el conjunto de datos MNIST. El conjunto de datos se descarga automáticamente la primera vez que se llama a esta función y se almacena en su directorio de inicio en ~/.keras/datasets/mnist.pkl.gz como un archivo de 15 megabytes. Para demostrar lo fácil que es cargar el conjunto de datos MNIST, primero escribiremos un pequeño script para descargar y visualizar las primeras 4 imágenes en el conjunto de datos de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#librerías\n",
    "\n",
    "from keras.datasets import mnist    # MNIST database\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# seleccionar entradas y salidas (etiquetas) de la base MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# dibujar 4 imágenes\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
    "# mostrar\n",
    "plt.show()\n",
    "print(\"X_train forma original\", X_train.shape)\n",
    "print(\"y_train forma original\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos también pueden ser visualizados de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (7,7) # agrando un poco la figura \n",
    "# ciclo para dibujar imágenes con su clase\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_train[i], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Class {}\".format(y_train[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pensemos lo siguiente:\n",
    "\n",
    "¿Realmente necesitamos un modelo complejo como una red neuronal convolucional para obtener los mejores resultados con MNIST? ¿Es posible obtener buenos resultados utilizando un modelo de red neuronal muy simple con una sola capa oculta? \n",
    "\n",
    "A continuación crearemos un modelo de Perceptrón multicapa simple que logre una tasa de error pequeña. Usaremos esto como base para la comparación con modelos de redes neuronales convolucionales más complejas. Comencemos importando las clases y funciones que necesitaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils\n",
    "\n",
    "seed = 5\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora cargamos la base de datos MNIST. El conjunto de datos de entrenamiento está estructurado como una matriz tridimensional de ancho de imagen, altura de imagen y produndidad. Para un modelo de perceptron multicapa, debemos reducir las imágenes a un vector de píxeles. En este caso, las imágenes de tamaño 28x28 serán vectores de entrada de 784 píxeles (28*28). Podemos hacer esta transformación fácilmente usando la función reshape () de la matriz NumPy. Los valores de píxel son enteros, por lo que los convertimos en valores de punto flotante para que podamos normalizarlos fácilmente en el próximo paso.\n",
    "\n",
    "Los valores de los píxeles están en escala de grises (entre 0 y 255). Siempre es una buena idea realizar un escalamiento (normalización) de valores de entrada cuando se usan modelos de redes neuronales. Debido a que la escala es bien conocida y se comporta bien, podemos normalizar muy rápidamente los valores de píxeles en el rango 0 y 1 dividiendo cada valor por el máximo de 255.\n",
    "\n",
    "Finalmente, la variable de salida es un número entero de 0 a 9. Este es un problema de clasificación multiclase ya que deben estar codificadas.  Es una buena práctica usar una codificación \"one hot encoding\", transformando el vector de enteros de clase en una matriz binaria. Podemos hacerlo fácilmente usando la función auxiliar np_utils.to_categorical() incorporada en Keras.\n",
    "\n",
    "0 -> [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "1 -> [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "2 -> [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datos\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# aplanar las imagenes de 28*28 a un vector de 784 \n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n",
    "\n",
    "# normalizar de 0-255 a 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "print(\"Forma de los datos de Training\", X_train.shape)\n",
    "print(\"Forma de los datos de Testing \", X_test.shape)\n",
    "\n",
    "YY_train=y_train # respaldar variables originales sin hot encoding\n",
    "YY_test=y_test\n",
    "# one hot \n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición y ajuste del del modelo de red neuronal simple\n",
    "\n",
    "Ahora estamos listos para crear nuestro modelo de red neuronal simple. Vamos a definir nuestro modelo en una función. Esto es útil si desea extender el ejemplo más adelante e intentar obtener un mejor desempeño.\n",
    "\n",
    "El modelo es una red neuronal simple con una capa oculta con el mismo número de neuronas que las entradas (784). Se utiliza una función de activación de rectificador ReLU para las neuronas en el capa oculta. Se utiliza una función de activación de softmax en la capa de salida para convertir las salidas en valores de probabilidad y permitir que se seleccione una clase de las 10, como la predicción de salida del modelo. La pérdida logarítmica se usa como la función de pérdida (crossentropy categórica). Para ajustar los pesos se utiliza el algoritmo de descenso de gradiente ADAM. A continuación se proporciona un resumen de la estructura de la red:\n",
    "\n",
    "<img src=\"./images_tutoriales/im1.png\">\n",
    "\n",
    "Ahora podemos ajustar y evaluar el modelo. El modelo se ajusta a 10 épocas con actualizaciones cada 200 imágenes (batch). Los datos de test se utilizan como el conjunto de datos de validación, lo que le permite ver la habilidad del modelo a medida que se entrena. Se utiliza un verbose=2 para reducir la salida a una línea para cada época de entrenamiento. Finalmente, el conjunto de datos de test se utiliza para evaluar el modelo y se imprime una tasa de error de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo_base\n",
    "def modelo_base():\n",
    "    # modelo\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, init='normal', activation='relu'))\n",
    "    model.add(Dense(num_classes, init='normal', activation='softmax'))\n",
    "    # Compilar\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# generar modelo\n",
    "model = modelo_base()\n",
    "\n",
    "# ajustar\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=10, batch_size=200, verbose=2)\n",
    "\n",
    "# evaluacion final \n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\" Error del modelo: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspeccionando la salida del sistema\n",
    "\n",
    "Siempre es una buena idea inspeccionar la salida y asegurarse de que todo se vea bien. Aquí veremos algunos ejemplos donde nuestro clasificador lo hace bien, y algunos ejemplos en los que se equivoca. Es importante destacar que se utilizan las variables originales (sin one hot encoding). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# predicción del modelo a los datos de test\n",
    "predicted_classes = model.predict_classes(X_test)\n",
    "\n",
    "# verificamos cuales son correctos y cuales no, usando las etiquetas y predicciones \n",
    "correct_indices = np.nonzero(predicted_classes == YY_test)[0]\n",
    "incorrect_indices = np.nonzero(predicted_classes != YY_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dibujemos algunas predicciones correctas e incorrectas\n",
    "plt.figure()\n",
    "for i, correct in enumerate(correct_indices[:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_test[correct].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], YY_test[correct]))\n",
    "    \n",
    "plt.figure()\n",
    "for i, incorrect in enumerate(incorrect_indices[:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[incorrect], YY_test[incorrect]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2. Redes Neuronales Convolucionales\n",
    "\n",
    "# Modelo de una CNN simple para MNIST\n",
    "\n",
    "Ahora que hemos visto cómo cargar el conjunto de datos MNIST y entrenar en él un modelo de Perceptrón multicapa simple, es hora de desarrollar una red neuronal convolucional o CNN, más sofisticada. Keras proporciona una gran capacidad para crear redes neuronales convolucionales. En esta sección, crearemos una CNN simple para MNIST que demuestra cómo utilizar todos los aspectos de una implementación moderna de CNN, incluidas las capas convolucionales, las capas de pooling y las capas de exclusión (dropout). El primer paso es importar las clases y funciones necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametros del experimento \n",
    "batch_size = 200\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, debemos cargar el conjunto de datos MNIST y remodelarlo para que sea adecuado para el entrenamiento de una CNN. En Keras, las capas utilizadas para las convoluciones bidimensionales esperan valores de píxel con las dimensiones [ancho] [altura][canales] . En el caso de RGB, los canales son 3 para los componentes rojo, verde y azul (3 entradas de imagen, una para cada color). En el caso de MNIST donde los valores de los canales están en escala de grises, la dimensión del píxel se establece en 1.\n",
    "\n",
    "Como antes, es una buena idea normalizar los valores de píxel en el rango 0 y 1 y hacer one hot para la variable de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# cambiar a [samples][fil][col][profundidad]\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# normalización\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# one hot\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición del modelo CNN \n",
    "\n",
    "A continuación vamos a definir nuestro modelo de red neuronal. Las redes neuronales convolucionales son más complejas que los perceptrones multicapa estándar, por lo que comenzaremos utilizando una estructura simple para utilizar todos los elementos del estado del arte y obtener resultados de vanguardia. A continuación se resume la arquitectura de la red.\n",
    "\n",
    "1. La primera capa oculta es una capa convolucional llamada Convolution2D. La capa tiene 32 mapas de características (32 filtros de convolución), de tamaño de 5x5 y una función de activación de rectificación (ReLU). Esta capa recibe las imágenes de entrada. \n",
    "\n",
    "2. A continuación, definimos una capa de agrupación (pooling) que toma el valor máximo denominado MaxPooling2D. Se configura con un tamaño de pooling de 2x2.\n",
    "\n",
    "3. La siguiente capa es una capa de regularización que utiliza el abandono de neuronas denominado dropout. Está configurado para excluir al azar el 20% de las neuronas en la capa para reducir el exceso de información y que el clasificador no memorice todos los ejemplos de entrenamiento.\n",
    "\n",
    "4. La siguiente es una capa que convierte los datos de la matriz 2D en un vector plano (Flatten). Permite que la salida sea procesada por capas estándar completamente conectadas (fully connected).\n",
    "\n",
    "5. A continuación, se utiliza una capa totalmente conectada (Dense) con 128 neuronas y una función de activación de rectificación ReLU.\n",
    "\n",
    "6. Finalmente, la capa de salida tiene 10 neuronas para las 10 clases y una función de activación de softmax para generar predicciones de probabilidad para cada clase.\n",
    "\n",
    "Como antes, el modelo se entrena utilizando la pérdida logarítmica multiclase y el algoritmo de descenso de gradiente ADAM. A continuación se proporciona una descripción de la estructura de la red.\n",
    "\n",
    "<img src=\"./images_tutoriales/im2.png\">\n",
    "\n",
    "Si se quiere visualizar la CNN, puede ser de ayuda el siguiente esquema. Notar que el esquema posee diferentes parámetros de filtros, no posee dropout, etc. Es solo para visualizar el proceso de la CNN. \n",
    "\n",
    "<img src=\"./images_tutoriales/im3.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo\n",
    "def modelo_cnn():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(5, 5), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = modelo_cnn()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo puede ser resumido a continuación.\n",
    "\n",
    "Luego, evaluamos el modelo de la misma manera que antes con el Perceptrón multicapa. La CNN es más de 10 épocas con un tamaño de lote de 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochss = range(1, len(acc) + 1)\n",
    "plt.plot(epochss, acc, 'r--o', label='Training acc')\n",
    "plt.plot(epochss, val_acc, 'b--x', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochss, loss, 'r--o', label='Training loss')\n",
    "plt.plot(epochss, val_loss, 'b--x', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(\"CNN Error: %.2f%%\" % (100-score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Código completo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# parametros del experimento \n",
    "batch_size = 200\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# load data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# cambiar a [samples][fil][col][profundidad]\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# normalización\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# one hot\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# modelo\n",
    "def modelo_cnn():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(5, 5), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compilar el modelo\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = modelo_cnn()\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "\n",
    "plt.figure()\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochss = range(1, len(acc) + 1)\n",
    "plt.plot(epochss, acc, 'r--o', label='Training acc')\n",
    "plt.plot(epochss, val_acc, 'b--x', label='Validation acc')\n",
    "plt.title('Accuracy: Entrenamiento y Validación')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochss, loss, 'r--o', label='Training loss')\n",
    "plt.plot(epochss, val_loss, 'b--x', label='Validation loss')\n",
    "plt.title('Loss: Entrenamiento y Validación')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(\"CNN Error: %.2f%%\" % (100-score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mejorando la CNN aumentando el número de capas ocultas\n",
    "\n",
    "Ahora que hemos visto cómo crear una CNN simple, echemos un vistazo a un modelo capaz de obtener resultados cercanos al estado del arte. Importamos las clases y las funciones, luego cargamos y preparamos los datos de la misma manera que en el ejemplo anterior. Esta vez, hemos definido una arquitectura CNN más grande con capas convolucionales adicionales, de agrupación máxima (pooling) y capas totalmente conectadas. La topología de la red se puede resumir de la siguiente manera.\n",
    "\n",
    "1. Capa convolucional con 30 mapas de características de tamaño 5 x 5.\n",
    "2. Agrupar (pooling) la capa tomando el máximo de 2 x 2 parches.\n",
    "3. Capa convolucional con 15 mapas de características de tamaño 3 x 3.\n",
    "4. Agrupar (pooling) la capa tomando el máximo de 2 x 2 parches.\n",
    "5. Capa de abandono (dropout) con una probabilidad del 20%.\n",
    "6. Aplanar la capa (flatten).\n",
    "7. Capa totalmente conectada con 128 neuronas y activación de rectificador.\n",
    "8. Capa totalmente conectada con 50 neuronas y activación de rectificador.\n",
    "9. Output layer.\n",
    "\n",
    "A continuación se proporciona una descripción de esta estructura de red más grande:\n",
    "\n",
    "<img src=\"./images_tutoriales/im4.png\">\n",
    "\n",
    "Al igual que en los dos experimentos anteriores, el modelo es más de 10 épocas con un tamaño de lote de 200.\n",
    "\n",
    "Al ejecutar el ejemplo se imprime la precisión en los conjuntos de datos de entrenamiento y validación de cada época y una tasa de error de clasificación final. El modelo tarda unos 60 segundos en ejecutarse por época en una CPU moderna. Este modelo ligeramente más grande que el anterior logra la tasa de error de clasificación respetable del 0,8%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# parametros del experimento \n",
    "batch_size = 200\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# load data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# cambiar a [samples][fil][col][profundidad]\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# normalización\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "YY_train=y_train # respaldar variables originales sin hot encoding\n",
    "YY_test=y_test\n",
    "\n",
    "XX_train=x_train # respaldar variables originales sin hot encoding\n",
    "XX_test=x_test\n",
    "\n",
    "\n",
    "# one hot\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# modelo\n",
    "def modelo_cnn():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(30, kernel_size=(5, 5),activation='relu',input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.20))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compilar el modelo\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = modelo_cnn()\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochss = range(1, len(acc) + 1)\n",
    "plt.plot(epochss, acc, 'r--o', label='Training acc')\n",
    "plt.plot(epochss, val_acc, 'b--x', label='Validation acc')\n",
    "plt.title('Accuracy: Entrenamiento y Validación')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochss, loss, 'r--o', label='Training loss')\n",
    "plt.plot(epochss, val_loss, 'b--x', label='Validation loss')\n",
    "plt.title('Loss: Entrenamiento y Validación')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(\"CNN Error: %.2f%%\" % (100-score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revisión del clasificador CNN y puntajes\n",
    "\n",
    "Observemos ahora el rendimiento de la red. Para esto vamos a tomar los datos de test que la red no ha visto y veremos que predicciones realiza antes entradas aleatoreas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "index = np.random.choice(list(range(len(x_test))), 1)[0]\n",
    "im = x_test[index]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "print('la imagen de test:')\n",
    "plt.imshow(im[..., 0], cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "print('la clase que predice la red es: ', np.argmax(model.predict(np.reshape(im, [1,28,28,1])), -1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# predicciones de nuestra CNN con los datos de test\n",
    "predicted_classes = model.predict_classes(x_test)\n",
    "\n",
    "# revisemos cuales prediccioines son correctas e incorrectas\n",
    "correct_indices = np.nonzero(predicted_classes == YY_test)[0]\n",
    "incorrect_indices = np.nonzero(predicted_classes != YY_test)[0]\n",
    "\n",
    "plt.figure()\n",
    "for i, correct in enumerate(correct_indices[:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(x_test[correct].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], YY_test[correct]))\n",
    "    \n",
    "plt.figure()\n",
    "for i, incorrect in enumerate(incorrect_indices[:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(x_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[incorrect], YY_test[incorrect]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matriz de confusión y reportes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score \n",
    "\n",
    "# classification report\n",
    "print('Reporte de Clasificación:')\n",
    "print(classification_report( YY_test,predicted_classes))\n",
    "\n",
    "# confusion matrix \n",
    "cm = confusion_matrix(YY_test,predicted_classes)\n",
    "\n",
    "# mostrar los resultados\n",
    "print('Matriz de confusión:')\n",
    "print(cm)\n",
    "\n",
    "# Print f1, precision, and recall scores\n",
    "print('Precision:')\n",
    "print(precision_score(YY_test, predicted_classes , average=\"macro\"))\n",
    "print('Recall:')\n",
    "print(recall_score(YY_test, predicted_classes , average=\"macro\"))\n",
    "print('F1:')\n",
    "print(f1_score(YY_test, predicted_classes, average=\"macro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
